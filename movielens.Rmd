---
title: "Movie lens Rating Prediction Project"
author: "Sandeep Kumar"
date: "1/29/2022"
output: pdf_document
---

## Introduction

Recommendation systems use ratings that users have given to items to make specific recommendations.Items for which a high rating is predicted for a given user are then recommended to that user. 

Recommendation systems are one of the most used models in machine learning algorithms.For this project we will focus on create a movie recommendation system using the 10M version of MovieLens dataset, collected by GroupLens Research.


## Goal of the project

The goal in this project is to train a machine learning algorithm that predicts user ratings using the inputs of a provided subset  (edx dataset provided by the staff) to predict movie ratings in a provided validation set.

The function that computes the RMSE for vectors of ratings and their corresponding predictors will be the following:
$$ RMSE = \sqrt{\frac{1}{N}\displaystyle\sum_{u,i} (\hat{y}_{u,i}-y_{u,i})^{2}} $$

```{r RMSE_function1, echo = FALSE}
RMSE <- function(predicted_ratings, true_ratings){
  sqrt(mean((predicted_ratings - true_ratings)^2))
}
```


Finally, the best resulting model will be used to predict the movie ratings.


## Dataset

The MovieLens dataset is automatically downloaded

• [MovieLens 10M dataset] https://grouplens.org/datasets/movielens/10m/

• [MovieLens 10M dataset - zip file] http://files.grouplens.org/datasets/movielens/ml-10m.zip


```{r, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
#############################################################
# Create edx set, validation set, and submission file
#############################################################
# Note: this process could take a couple of minutes for loading required package: tidyverse and package caret
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)
ratings <- read.table(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                      col.names = c("userId", "movieId", "rating", "timestamp"))
movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                           title = as.character(title),
                                           genres = as.character(genres))
movielens <- left_join(ratings, movies, by = "movieId")
```
  

```{r, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
# The Validation subset will be 10% of the MovieLens data.
set.seed(1)
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]
#Make sure userId and movieId in validation set are also in edx subset:
validation <- temp %>%
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")
# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)
rm(dl, ratings, movies, test_index, temp, movielens, removed)
```


The MovieLens dataset will be splitted into 2 subsets that will be the “edx”, a training subset to train the algorithm, and “validation” a subset to test the movie ratings.

Algorithm development is to be carried out on the "edx" subset only, as "validation" subset will be used to test the final algorithm.
\pagebreak


## Modelling Approach

Compute the RMSE, defined as follows:

$$ RMSE = \sqrt{\frac{1}{N}\displaystyle\sum_{u,i} (\hat{y}_{u,i}-y_{u,i})^{2}} $$


The RMSE is our measure of model accuracy.
We can interpret the RMSE similarly to a standard deviation: it is the typical error we make when predicting a movie rating. If its result is larger than 1, it means that our typical error is larger than one star, which is not a good result.

```{r RMSE_function2, echo = TRUE}
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}
```


### I. Average rating model

The averge rating model predicts the same rating for all movies, so we compute the dataset’s mean rating.
We know that the estimate that minimize the RMSE is the least square estimate the average of all ratings:
The expected rating of the underlying data set is between 3 and 4.

```{r, echo = TRUE}
mr <- mean(edx$rating)
mr
```


If we predict all unknown ratings with mr, we obtain the first naive RMSE:

```{r naive_rmse, echo = TRUE}
naive_rmse <- RMSE(validation$rating, mr)
naive_rmse
```


Here, we represent results table with the first RMSE:

```{r rmse_results1, echo = TRUE}
rmse_results <- data_frame(method = "Average movie rating model", RMSE = naive_rmse)
rmse_results %>% knitr::kable()
```

The above RMSE give us our baseline RMSE to compare with next modelling approaches.


### II.  Movie effect model

We know that some movies are just generally rated higher than others. Higher ratings are mostly linked to popular movies among users and the opposite is true for unpopular movies. We compute the estimated deviation of each movies’ mean rating from the total mean of all movies $\mu$. The resulting variable is called "b" ( as bias ) for each movie "i" $b_{i}$, that represents average ranking for movie $i$:
$$Y_{u, i} = \mu +b_{i}+ \epsilon_{u, i}$$

The histogram is left skewed, implying that more movies have negative effects


```{r Number_of_movies_with_the computed_b_i, echo = TRUE, fig.height=3, fig.width=4}
movie_avgs <- edx %>%
  group_by(movieId) %>%
  summarize(b_i = mean(rating - mr))
movie_avgs %>% qplot(b_i, geom ="histogram", bins = 10, data = ., color = I("black"),
ylab = "Number of movies", main = "Number of movies with the computed b_i")
```


This is called the penalty term movie effect.

```{r predicted_ratings, echo = TRUE}
predicted_ratings <- mr +  validation %>%
  left_join(movie_avgs, by='movieId') %>%
  pull(b_i)
model_s_rmse <- RMSE(predicted_ratings, validation$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie effect model",  
                                     RMSE = model_s_rmse ))
rmse_results %>% knitr::kable()
```


So we have predicted movie rating based on the fact that movies are rated differently by adding the computed $b_{i}$ to $\mr$.

We can see an improvement but this model does not consider the individual user rating effect.


### III. Regularized effect model

The use of the regularization permits to penalize the aspects of movies with very few ratings and in some users that only rated a very small number of movies. We should find the value of lambda (that is a tuning parameter) that will minimize the RMSE.


```{r lambdas, echo = TRUE}
lambdas <- seq(0, 10, 0.25)
rmses <- sapply(lambdas, function(l){
  
  mr <- mean(edx$rating)
  
  b_i <- edx %>% 
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mr)/(n()+l))
  
  b_u <- edx %>% 
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mr)/(n()+l))
  
  predicted_ratings <- 
    validation %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(pred = mr + b_i + b_u) %>%
    pull(pred)
  
  return(RMSE(predicted_ratings, validation$rating))
})
```

For the full model, the optimal lambda is:

```{r min_lambda, echo = TRUE}
  lambda <- lambdas[which.min(rmses)]
lambda
```

For the full model, the optimal lambda is: 5.25

The new results will be:


```{r rmse_results2, echo = TRUE}
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Regularized effect model",  
                                     RMSE = min(rmses)))
rmse_results %>% knitr::kable()
```

# Results

The RMSE values of all the 3 represented models are the following:

```{r rmse_results3, echo = TRUE}
rmse_results %>% knitr::kable()
```

We therefore found the lowest value of RMSE that is 0.8648170.

# Conclusion

The regularized model including the effect of user is characterized by the lower RMSE value and is hence the optimal model to use for the present project.
The optimal model characterised by the lowest RMSE value (0.8648170) lower than the initial evaluation criteria (0.8775) given by the goal of the present project.